<html>

<head>

<link href='https://fonts.googleapis.com/css?family=Cabin:600|Nunito:400,300' rel='stylesheet' type='text/css'>

<title>Accepted Papers: Language Grounding for Robotics</title>

<link rel="stylesheet" href="workshop.css" type="text/css">

<style>
body {
background-color:#ffffff;
overflow-y: scroll;
}
hr.style2 {
	border-top: 3px double #8c8b8b;
}
</style>

</head>

<body>

<div class="center2">

<table class="center1">

<tr class="emptymargins">

<td>

<br><br>

</td>

</tr>

<tr class="emptymargins">

<td class="emptymargins">

<table class="emptymargins">

<tr class="emptymargins">

<td class="headername">

Accepted Papers: Language Grounding for Robotics

</td>

<td class="headerright">


Workshop at <a href="http://acl2017.org/">ACL 2017</a><br>
Vancouver, Canada<br>
Dates: August 3, 2017<br>


</td>

</tr>

</table>

</td>

</tr>

<tr>

<td class="menumainspace"><br></td>

</tr>





<tr class="emptymargins">

<td class="emptymargins">

<table class="mainpane2">

<tr class="emptymargins">

<td class="maintext2" style="padding-left: 35px; padding-right: 35px;">


<br>

The 1st workshop on <a href="https://robonlp2017.github.io/">Language Grounding for Robotics at ACL 2017</a> accepted the following papers:
**Regular Workshop Papers**

Interpreting Action-Based and Goal-Based Instructions in a Mobile-Manipulator Domain
Siddharth Karamcheti, Edward Clem Williams, Dilip Arumugam, Mina Rhee, Nakul Gopalan, Lawson L.S. Wong and Stefanie Tellex

Guiding Interaction Behaviors for Multi-modal Grounded Language Learning 
Jesse Thomason, Jivko Sinapov and Raymond Mooney

Grounding Language for Interactive Task Learning 
Peter Lindes, Aaron Mininger, James R. Kirk and John E. Laird

Towards Problem Solving Agents that Communicate and Learn 
Anjali Narayan-Chen, Colin Graber, Mayukh Das, Md Rakibul Islam, Soham Dan, Sriraam Natarajan, Janardhan Rao Doppa, Julia Hockenmaier, Martha Palmer and Dan Roth

Learning how to learn: an adaptive dialogue agent for incrementally learning visually grounded word meanings 
Yanchao Yu, Arash Eshghi and Oliver Lemon

Exploring Variation of Natural Human Commands to a Robot in a Collaborative Navigation Task 
Matthew Marge, Claire Bonial, Ashley Foots, Cory Hayes, Cassidy Henry, Kimberly Pollard, Ron Artstein, Clare Voss and David Traum

Structured Learning for Context-aware Spoken Language Understanding of Robotic Commands 
Andrea Vanzo, Danilo Croce, Roberto Basili and Daniele Nardi

Natural Language Grounding and Grammar Induction for Robotic Manipulation Commands 
Muhannad Alomari, Paul Duckworth, Majd Hawasly, David C. Hogg and Anthony G. Cohn 

Communication with Robots using Multilayer Recurrent Networks 
Bedřich Pišl and David Mareček

Grounding Symbols in Multi-Modal Instructions 
Yordan Hristov, Svetlin Penkov, Alex Lascarides and Subramanian Ramamoorthy

Are distributional representations ready for the real world? Evaluating word vectors for grounded perceptual meaning 
Li Lucy and Jon Gauthier

Sympathy Begins with a Smile, Intelligence Begins with a Word: Use of Multimodal Features in Spoken Human-Robot Interaction 
Jekaterina Novikova, Christian Dondrup, Ioannis Papaioannou and Oliver Lemon


**Cross-Submission Papers**

Gated-Attention Architectures for Task-Oriented Language Grounding
Devendra Singh Chaplot, Kanthashree Mysore Sathyendra, Rama Kumar Pasumarthi, Dheeraj Rajagopal and Ruslan Salakhutdinov

Procedural Semantics - A Case Study in Locative Spatial Language (IROS 2015)
Michael Spranger

State Based Grounded Verb Semantic and Interactive Acquisition (ACL2017)
Lanbo She and Joyce Chai

Temporal Grounding Graphs for Language Understanding with Accrued Visual-Linguistic Context (IJCAI 2017)
Rohan Paul, Andrei Barbu, Sue Felshin, Boris Katz and Nicholas Roy

Grounding of Human Environments and Activities for Autonomous Robots (IJCAI 2017)
Muhannad Alomari, Paul Duckworth, Majd Hawasly, Nils Bore, David Hogg and Anthony Cohn

A Corpus of Natural Language for Visual Reasoning (ACL 2017)
Alane Suhr, Mike Lewis, James Yeh and Yoav Artzi

Situation Recognition: Visual Semantic Role Labeling for Image Understanding (CVPR 2016)
Mark Yatskar, Luke Zettlemoyer and Ali Farhadi

</td>

</tr>

</table>

</td>

</tr>

</table>

</div>

<br>

<br>

</body>

</html>
