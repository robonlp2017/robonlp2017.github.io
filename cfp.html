<html>

<head>

<link href='https://fonts.googleapis.com/css?family=Cabin:600|Nunito:400,300' rel='stylesheet' type='text/css'>

<title>Call for Papers: Language Grounding for Robotics</title>

<link rel="stylesheet" href="workshop.css" type="text/css">

<style>
body {
background-color:#ffffff;
overflow-y: scroll;
}
hr.style2 {
	border-top: 3px double #8c8b8b;
}
</style>

</head>

<body>

<div class="center2">

<table class="center1">

<tr class="emptymargins">

<td>

<br><br>

</td>

</tr>

<tr class="emptymargins">

<td class="emptymargins">

<table class="emptymargins">

<tr class="emptymargins">

<td class="headername">

Call for Papers: Language Grounding for Robotics

</td>

<td class="headerright">


Workshop at <a href="http://acl2017.org/">ACL 2017</a><br>
Vancouver, Canada<br>
Dates: August 3, 2017<br>


</td>

</tr>

</table>

</td>

</tr>

<tr>

<td class="menumainspace"><br></td>

</tr>





<tr class="emptymargins">

<td class="emptymargins">

<table class="mainpane2">

<tr class="emptymargins">

<td class="maintext2" style="padding-left: 35px; padding-right: 35px;">


<br>

<a name="overview"></a>
<span class="maintextheader1"><strong>Overview</strong>:</span><br><br>

The embodied, task-oriented aspect of language grounding is an important and timely next research direction following the exciting recent progress in visual language grounding. To realize the long-term goal of robots that we can converse with in our homes, offices, hospitals, and warehouses, it is essential that we develop new techniques for linking language to action in the real world. Can we give instructions to robotic agents to assist with navigation and manipulation tasks in remote settings? Can we talk to robots about the surrounding visual world, and help them interactively learn the language needed to finish a task? We hope to learn about (and begin to answer) these questions in the proposed workshop on Language Grounding for Robotics.

<br><br>

This workshop aims to bring together members of the NLP, robotics, and vision communities to focus on the much-needed task-oriented aspect of language grounding. The confirmed invited speakers, program committee, and organizing committee (see below) consist of researchers who belong to these communities, and who work at the intersection of language, robotics, and vision.

<br><br>

Topics of interest for this workshop include, but are not limited to:
<font size="2">
<br>
<ul>
<li> Aligning and Translating Language to Situated Actions </li>
<li> Simulated and Real World Situations </li>
<li> Instructions for Navigation </li>
<li> Instructions for Articulation </li>
<li> Instructions for Manipulation </li>
<li> Skill Learning via Interactive Dialogue </li>
<li> Language Learning via Grounded Dialogue </li>
<li> Language Generation for Embodied Tasks </li>
<li> Grounded Knowledge Representations </li>
<li> Mapping Language and World </li>
<li> Grounded Reinforcement Learning </li>
<li> Language-based Game Playing for Grounding </li>
<li> Structured and Deep Learning Models for Embodied Language </li>
<li> New Datasets for Embodied Language </li>
<li> Better Evaluation Metrics for Embodied Language </li>
</ul>
</font>

Format: The proposed format is 6-7 invited speaker talks, several oral paper talks, poster sessions, and a concluding panel with 4-5 of the invited speakers to discuss the concrete future directions, challenges, and collaboration opportunities for this important topic.

<br><br>

<a name="dates"></a>
<span class="maintextheader1"><strong>Important Dates</strong>:</span><br><br>
Deadline for submission: 21 April 2017 <br>
Notification of acceptance: 19 May 2017 <br>
Deadline for camera-ready version: 26 May 2017 <br>
Early registration deadline (ACL'17): TBA <br>
Workshop Date: 3 August 2017<br>

<br><br>




</td>

</tr>

</table>

</td>

</tr>

</table>

</div>

<br>

<br>

</body>

</html>

